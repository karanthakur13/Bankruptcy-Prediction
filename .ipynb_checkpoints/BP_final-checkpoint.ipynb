{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM3goFHzgQKj"
   },
   "source": [
    "**1. Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIxksC-Ceq7g",
    "outputId": "f2817459-c026-4cec-a8db-2dccfb8a3ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fancyimpute\n",
      "  Using cached fancyimpute-0.7.0.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting knnimpute>=0.1.0 (from fancyimpute)\n",
      "  Using cached knnimpute-0.1.0.tar.gz (8.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fancyimpute) (1.3.0)\n",
      "Collecting cvxpy (from fancyimpute)\n",
      "  Using cached cvxpy-1.3.2-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "Collecting cvxopt (from fancyimpute)\n",
      "  Using cached cvxopt-1.3.1-cp311-cp311-macosx_13_0_arm64.whl (11.1 MB)\n",
      "Collecting pytest (from fancyimpute)\n",
      "  Using cached pytest-7.4.0-py3-none-any.whl (323 kB)\n",
      "Collecting nose (from fancyimpute)\n",
      "  Using cached nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from knnimpute>=0.1.0->fancyimpute) (1.25.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=0.24.2->fancyimpute) (3.2.0)\n",
      "Collecting osqp>=0.4.1 (from cvxpy->fancyimpute)\n",
      "  Using cached osqp-0.6.3.tar.gz (228 kB)\n",
      "  Installing build dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[115 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting oldest-supported-numpy\n",
      "  \u001b[31m   \u001b[0m   Using cached oldest_supported_numpy-2022.11.19-py3-none-any.whl (4.9 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools>=40.8.0\n",
      "  \u001b[31m   \u001b[0m   Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting wheel\n",
      "  \u001b[31m   \u001b[0m   Using cached wheel-0.41.0-py3-none-any.whl (64 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools_scm>=6.2\n",
      "  \u001b[31m   \u001b[0m   Using cached setuptools_scm-7.1.0-py3-none-any.whl (43 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting qdldl\n",
      "  \u001b[31m   \u001b[0m   Using cached qdldl-0.1.7.post0.tar.gz (70 kB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Installing backend dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing backend dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m Collecting numpy==1.23.2 (from oldest-supported-numpy)\n",
      "  \u001b[31m   \u001b[0m   Using cached numpy-1.23.2-cp311-cp311-macosx_11_0_arm64.whl (13.3 MB)\n",
      "  \u001b[31m   \u001b[0m Collecting packaging>=20.0 (from setuptools_scm>=6.2)\n",
      "  \u001b[31m   \u001b[0m   Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting typing-extensions (from setuptools_scm>=6.2)\n",
      "  \u001b[31m   \u001b[0m   Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting scipy>=0.13.2 (from qdldl)\n",
      "  \u001b[31m   \u001b[0m   Using cached scipy-1.11.1-cp311-cp311-macosx_12_0_arm64.whl (29.5 MB)\n",
      "  \u001b[31m   \u001b[0m Building wheels for collected packages: qdldl\n",
      "  \u001b[31m   \u001b[0m   Building wheel for qdldl (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Building wheel for qdldl (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   error: subprocess-exited-with-error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   × Building wheel for qdldl (pyproject.toml) did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   │ exit code: 1\n",
      "  \u001b[31m   \u001b[0m   ╰─> [72 lines of output]\n",
      "  \u001b[31m   \u001b[0m       running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m       running build\n",
      "  \u001b[31m   \u001b[0m       running build_ext\n",
      "  \u001b[31m   \u001b[0m       Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m         File \"<string>\", line 81, in build_extensions\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 465, in check_output\n",
      "  \u001b[31m   \u001b[0m           return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 546, in run\n",
      "  \u001b[31m   \u001b[0m           with Popen(*popenargs, **kwargs) as process:\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 1022, in __init__\n",
      "  \u001b[31m   \u001b[0m           self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 1899, in _execute_child\n",
      "  \u001b[31m   \u001b[0m           raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "  \u001b[31m   \u001b[0m       FileNotFoundError: [Errno 2] No such file or directory: 'cmake'\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m       Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m           main()\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m           json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m           return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 416, in build_wheel\n",
      "  \u001b[31m   \u001b[0m           return self._build_with_temp_dir(['bdist_wheel'], '.whl',\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 401, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m           self.run_setup()\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 338, in run_setup\n",
      "  \u001b[31m   \u001b[0m           exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m         File \"<string>\", line 113, in <module>\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 107, in setup\n",
      "  \u001b[31m   \u001b[0m           return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m           return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m           dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m           self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m           super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m           cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/normal/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 346, in run\n",
      "  \u001b[31m   \u001b[0m           self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m           self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m           super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m           cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m           self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m           self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 1234, in run_command\n",
      "  \u001b[31m   \u001b[0m           super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m           cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 84, in run\n",
      "  \u001b[31m   \u001b[0m           _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m         File \"/private/var/folders/6s/b8cd55892x77v_wm7kwnjdx80000gn/T/pip-build-env-l09yqiu1/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m           self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m         File \"<string>\", line 83, in build_extensions\n",
      "  \u001b[31m   \u001b[0m       RuntimeError: CMake must be installed to build qdldl\n",
      "  \u001b[31m   \u001b[0m       [end of output]\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m   ERROR: Failed building wheel for qdldl\n",
      "  \u001b[31m   \u001b[0m Failed to build qdldl\n",
      "  \u001b[31m   \u001b[0m ERROR: Could not build wheels for qdldl, which is required to install pyproject.toml-based projects\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m [notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "  \u001b[31m   \u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install fancyimpute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1Bao2ZCbZBZ",
    "outputId": "1959322c-48c5-42b1-d303-4810f0e62f69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.25.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SQfA61fbZBZ",
    "outputId": "73769363-4c62-49c2-e90b-3f772fc8eae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: impyute in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.8)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from impyute) (1.25.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from impyute) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from impyute) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->impyute) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->impyute) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install impyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1Bf3qJmZIyI"
   },
   "outputs": [],
   "source": [
    "# To supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# Basic Libraries for Data organization, Statistical operations and Plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# For loading .arff files\n",
    "from scipy.io import arff\n",
    "# To analyze the type of missing data\n",
    "import missingno as msno\n",
    "# Library for performing k-NN and MICE imputations \n",
    "import fancyimpute\n",
    "# Library to perform Expectation-Maximization (EM) imputation\n",
    "import impyute as impy\n",
    "# To perform mean imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#To perform kFold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "# Formatted counter of class labels\n",
    "from collections import Counter\n",
    "# Ordered Dictionary\n",
    "from collections import OrderedDict\n",
    "# Library imbalanced-learn to deal with the data imbalance. To use SMOTE oversampling\n",
    "from imblearn.over_sampling import SMOTE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppi368gBgWYb"
   },
   "source": [
    "2. Importing and organizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "YuGypCj9e77U",
    "outputId": "ce86a1c3-dfdb-46bb-c28e-32ec20ef24a1"
   },
   "outputs": [],
   "source": [
    "d1= arff.loadarff('1year.arff')\n",
    "df1 = pd.DataFrame(d1[0])\n",
    "d2= arff.loadarff('2year.arff')\n",
    "df2 = pd.DataFrame(d2[0])\n",
    "d3= arff.loadarff('3year.arff')\n",
    "df3 = pd.DataFrame(d3[0])\n",
    "d4= arff.loadarff('4year.arff')\n",
    "df4 = pd.DataFrame(d4[0])\n",
    "d5= arff.loadarff('5year.arff')\n",
    "df5 = pd.DataFrame(d5[0])\n",
    "\n",
    "############################################################\n",
    "# Set the column headers from X1 ... X64 and the class label as Y, for all the 5 dataframes.\n",
    "def set_new_headers(dataframes):\n",
    "    cols = ['X' + str(i+1) for i in range(len(dataframes[0].columns)-1)]\n",
    "    cols.append('Y')\n",
    "    for df in dataframes:\n",
    "        df.columns = cols\n",
    "\n",
    "############################################################\n",
    "# dataframes is the list of pandas dataframes for the 5 year datafiles.  \n",
    "DF= pd.concat([df1,df2,df3,df4,df5],axis = 0)\n",
    "dataframes = [df1, df2, df3, df4, df5]\n",
    "\n",
    "# Set the new headers for the dataframes. The new headers will have the renamed set of feature (X1 to X64)\n",
    "set_new_headers(dataframes)    \n",
    "\n",
    "# print the first 5 rows of a dataset 'year1'\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6eRXHGegcO8"
   },
   "source": [
    "2.A Convert the columns types for the features to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pT83i0wfxbh"
   },
   "outputs": [],
   "source": [
    "# Convert the dtypes of all the columns (other than the class label columns) to float.\n",
    "def convert_columns_type_float(dfs):\n",
    "    for i in range(5):\n",
    "        index = 1\n",
    "        while(index<=63):\n",
    "            colname = dfs[i].columns[index]\n",
    "            col = getattr(dfs[i], colname)\n",
    "            dfs[i][colname] = col.astype(float)\n",
    "            index+=1\n",
    "            \n",
    "convert_columns_type_float(dataframes)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v_jfz8rgfar"
   },
   "source": [
    "2.B Convert the class label types to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9eVt-6ggI_C"
   },
   "outputs": [],
   "source": [
    "# The class labels for all the dataframes are originally in object type.\n",
    "# Convert them to int types\n",
    "def convert_class_label_type_int(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        col = getattr(dfs[i], 'Y')\n",
    "        dfs[i]['Y'] = col.astype(int)\n",
    "        \n",
    "convert_class_label_type_int(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDXbyQs3gkC2"
   },
   "source": [
    "3. Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7J5TafvgnHs"
   },
   "source": [
    "3.A Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ9WRtqCgLdd",
    "outputId": "393da800-314f-4c09-e0a8-b52e00559219"
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Get Clean dataframes by dropping all the rows which have missing values\n",
    "def drop_nan_rows(dataframes, verbose=False):\n",
    "    clean_dataframes = [df.dropna(axis=0, how='any') for df in dataframes]\n",
    "    if verbose:\n",
    "        for i in range(len(dataframes)):\n",
    "            print(str(i+1)+'year:','Original Length=', len(dataframes[i]), '\\tCleaned Length=', len(clean_dataframes[i]), '\\tMissing Data=', len(dataframes[i])-len(clean_dataframes[i]))\n",
    "    return clean_dataframes\n",
    "\n",
    "# Doing a quick analysis of how many missing values are there in each of the 5 dataframes\n",
    "nan_dropped_dataframes = drop_nan_rows(dataframes, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnwJK5Pzgqu9"
   },
   "source": [
    "3.A.a Generate Sparsity Matrix for the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_Rc-pgnFgpCB",
    "outputId": "129e8f40-9c6c-4b2e-fbea-49de9770fbae"
   },
   "outputs": [],
   "source": [
    "# generate the sparsity matrix (figure) for all the dataframes\n",
    "def generate_sparsity_matrix(dfs):\n",
    "    for i in range(5):\n",
    "        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n",
    "        msno.matrix(dfs[i][missing_df_i], figsize=(20,5))\n",
    "\n",
    "generate_sparsity_matrix(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yg70KUL4bZBc",
    "outputId": "c5f68528-ddc0-447b-faad-03e67f1e3c47"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data_keys=list(DF.columns)\n",
    "le=LabelEncoder()\n",
    "#class /label encoding\n",
    "DF['class']= le.fit_transform(DF['class'])\n",
    "print(DF['class'].unique())\n",
    "\n",
    "\n",
    "X= DF.drop('class',axis=1)\n",
    "Y= DF['class']\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imp = imputer.fit_transform(X)\n",
    "classifier = lgb.LGBMClassifier()\n",
    "\n",
    "#Select best feature \n",
    "rfe = RFE(classifier,n_features_to_select=12)\n",
    "rfe = rfe.fit(X_imp,Y)\n",
    "#Summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "pd.DataFrame(X_imp,columns=data_keys[:len(data_keys)-1]).columns[rfe.support_]\n",
    "X_imp= pd.DataFrame(X_imp,columns=data_keys[:len(data_keys)-1])\n",
    "X_fs= X_imp[X_imp.columns[rfe.support_]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDcLlbYBbZBc",
    "outputId": "d7e3b994-e385-450b-a6f4-ef4209068e4b"
   },
   "outputs": [],
   "source": [
    "X_imp.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViyqJP9agvzs"
   },
   "source": [
    "3.A.b Generate Heat Map for the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SnLqmOobZBc"
   },
   "outputs": [],
   "source": [
    "df1= df1.drop(['X1', 'X2', 'X3', 'X4', 'X6', 'X7', 'X8'],axis=1) \n",
    "df1= df1.drop(['X10', 'X11', 'X12', 'X13', 'X14', 'X15','X16'],axis=1)\n",
    "df1= df1.drop (['X18', 'X19', 'X21','X23','X25', 'X26',  'X28'],axis=1)\n",
    "df1= df1.drop (['X31', 'X32', 'X33', 'X35', 'X36','X37', 'X38'],axis=1) \n",
    "df1= df1.drop (['X42', 'X43', 'X45', 'X47', 'X48', 'X49', 'X50'],axis=1)\n",
    "df1= df1.drop(['X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X59'],axis=1)\n",
    "df1= df1.drop(['X61', 'X62', 'X63', 'X64'],axis=1)\n",
    "df1=df1.drop(['X9', 'X17', 'X30',  'X41','X51', 'X60'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7WrzIyTbZBc"
   },
   "outputs": [],
   "source": [
    "df2= df2.drop(['X1', 'X2', 'X3', 'X4', 'X6', 'X7', 'X8'],axis=1) \n",
    "df2= df2.drop(['X10', 'X11', 'X12', 'X13', 'X14', 'X15','X16'],axis=1)\n",
    "df2= df2.drop (['X18', 'X19', 'X21','X23','X25', 'X26',  'X28'],axis=1)\n",
    "df2= df2.drop (['X31', 'X32', 'X33', 'X35', 'X36','X37', 'X38'],axis=1) \n",
    "df2= df2.drop (['X42', 'X43', 'X45', 'X47', 'X48', 'X49', 'X50'],axis=1)\n",
    "df2= df2.drop(['X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X59'],axis=1)\n",
    "df2= df2.drop(['X61', 'X62', 'X63', 'X64'],axis=1)\n",
    "df2=df2.drop(['X9', 'X17', 'X30',  'X41','X51', 'X60'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9Ma_d-DbZBd"
   },
   "outputs": [],
   "source": [
    "df3= df3.drop(['X1', 'X2', 'X3', 'X4', 'X6', 'X7', 'X8'],axis=1) \n",
    "df3= df3.drop(['X10', 'X11', 'X12', 'X13', 'X14', 'X15','X16'],axis=1)\n",
    "df3= df3.drop (['X18', 'X19', 'X21','X23','X25', 'X26',  'X28'],axis=1)\n",
    "df3= df3.drop (['X31', 'X32', 'X33', 'X35', 'X36','X37', 'X38'],axis=1) \n",
    "df3= df3.drop (['X42', 'X43', 'X45', 'X47', 'X48', 'X49', 'X50'],axis=1)\n",
    "df3= df3.drop(['X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X59'],axis=1)\n",
    "df3= df3.drop(['X61', 'X62', 'X63', 'X64'],axis=1)\n",
    "df3=df3.drop(['X9', 'X17', 'X30',  'X41','X51', 'X60'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shsPrXqzbZBd"
   },
   "outputs": [],
   "source": [
    "df4= df4.drop(['X1', 'X2', 'X3', 'X4', 'X6', 'X7', 'X8'],axis=1) \n",
    "df4= df4.drop(['X10', 'X11', 'X12', 'X13', 'X14', 'X15','X16'],axis=1)\n",
    "df4= df4.drop (['X18', 'X19', 'X21','X23','X25', 'X26',  'X28'],axis=1)\n",
    "df4= df4.drop (['X31', 'X32', 'X33', 'X35', 'X36','X37', 'X38'],axis=1) \n",
    "df4= df4.drop (['X42', 'X43', 'X45', 'X47', 'X48', 'X49', 'X50'],axis=1)\n",
    "df4= df4.drop(['X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X59'],axis=1)\n",
    "df4= df4.drop(['X61', 'X62', 'X63', 'X64'],axis=1)\n",
    "df4=df4.drop(['X9', 'X17', 'X30',  'X41','X51', 'X60'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJNY0cQtbZBd"
   },
   "outputs": [],
   "source": [
    "df5= df5.drop(['X1', 'X2', 'X3', 'X4', 'X6', 'X7', 'X8'],axis=1) \n",
    "df5= df5.drop(['X10', 'X11', 'X12', 'X13', 'X14', 'X15','X16'],axis=1)\n",
    "df5= df5.drop (['X18', 'X19', 'X21','X23','X25', 'X26',  'X28'],axis=1)\n",
    "df5= df5.drop (['X31', 'X32', 'X33', 'X35', 'X36','X37', 'X38'],axis=1) \n",
    "df5= df5.drop (['X42', 'X43', 'X45', 'X47', 'X48', 'X49', 'X50'],axis=1)\n",
    "df5= df5.drop(['X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X59'],axis=1)\n",
    "df5= df5.drop(['X61', 'X62', 'X63', 'X64'],axis=1)\n",
    "df5=df5.drop(['X9', 'X17', 'X30',  'X41','X51', 'X60'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "jsAud5vrbZBd",
    "outputId": "8b366fd1-fe15-4614-d2bd-b2840506a618"
   },
   "outputs": [],
   "source": [
    "display(df5.iloc[5905])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHsFLaazbZBd"
   },
   "outputs": [],
   "source": [
    "dataframes = [df1, df2, df3, df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-MT0ptdIgsoZ",
    "outputId": "53cdccbd-9a06-4888-a09b-c63f49b8c549"
   },
   "outputs": [],
   "source": [
    "# generate the heatmap for all the dataframes\n",
    "def generate_heatmap(dfs):\n",
    "    for i in range(5):\n",
    "        missing_df_i = dfs[i].columns[dfs[i].isnull().any()].tolist()\n",
    "        msno.heatmap(dfs[i][missing_df_i], figsize=(20,20))\n",
    "        \n",
    "generate_heatmap(dataframes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25MR-0FTg7I9"
   },
   "source": [
    "3.B Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxyyKWcKg880"
   },
   "source": [
    "3.B.a Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ii-63dFgxtu"
   },
   "outputs": [],
   "source": [
    "def perform_mean_imputation(dfs):\n",
    "    # Construct an imputer with strategy as 'mean', to mean-impute along the columns\n",
    "    Imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    mean_Imp_dfs = [pd.DataFrame(Imp.fit_transform(df)) for df in dfs]\n",
    "    for i in range(len(dfs)):\n",
    "        mean_Imp_dfs[i].columns = dfs[i].columns   \n",
    "    return mean_Imp_dfs\n",
    "\n",
    "mean_Imp_dataframes = perform_mean_imputation(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoDKZ15QzTd7"
   },
   "source": [
    "3.B.b k-Nearest Neighbors (k-NN) Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Azn8VQzVhHVR",
    "outputId": "01816741-d359-42a5-a498-217e057fd129"
   },
   "outputs": [],
   "source": [
    "def perform_knn_imputation(dfs):\n",
    "    knn_imputed_datasets = [fancyimpute.KNN(k=100,verbose=True).fit_transform(dfs[i]) for i in range(len(dfs))]\n",
    "    return [pd.DataFrame(data=knn_imputed_datasets[i]) for i in range(len(dfs))]\n",
    "    \n",
    "knn_Imp_dataframes = perform_knn_imputation(dataframes)\n",
    "set_new_headers(knn_Imp_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-rTyhenFgjt"
   },
   "outputs": [],
   "source": [
    "imputed_dataframes_dictionary = OrderedDict()\n",
    "imputed_dataframes_dictionary['Mean'] = mean_Imp_dataframes\n",
    "imputed_dataframes_dictionary['k-NN'] = knn_Imp_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUijcUODZ8Kl",
    "outputId": "c4c16dad-7c26-4e54-f97e-85b5ed70909d"
   },
   "outputs": [],
   "source": [
    "def check_data_imbalance(dfs):\n",
    "    for i in range(len(dfs)):\n",
    "        print('Dataset: '+str(i+1)+'year')\n",
    "        print(dfs[i].groupby('Y').size())\n",
    "        minority_percent = (dfs[i]['Y'].tolist().count(1) / len(dfs[i]['Y'].tolist()))*100\n",
    "        print('Minority (label 1) percentage: '+  str(minority_percent) + '%')\n",
    "        print('-'*64)\n",
    "        \n",
    "check_data_imbalance(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mo2PEQ_GZ-Wa",
    "outputId": "647f0092-99a0-4c23-ac73-8a151ba9e684"
   },
   "outputs": [],
   "source": [
    "# Split the features and labels into separate dataframes for all the original dataframes\n",
    "def split_dataframes_features_labels(dfs):\n",
    "    feature_dfs = [dfs[i].iloc[:,0:12] for i in range(len(dfs))]\n",
    "    label_dfs = [dfs[i].iloc[:,12] for i in range(len(dfs))]\n",
    "    return feature_dfs, label_dfs\n",
    "\n",
    "# Performs the SMOTE oversampling fro given dataframes.\n",
    "def oversample_data_SMOTE(dfs, verbose=False):\n",
    "    smote = SMOTE(sampling_strategy='auto' , random_state=42, k_neighbors=10)\n",
    "    #Split the features and labels for each dataframe\n",
    "    feature_dfs, label_dfs = split_dataframes_features_labels(dfs)\n",
    "    resampled_feature_arrays = []\n",
    "    resampled_label_arrays = []\n",
    "    for i in range(len(dfs)):\n",
    "        if verbose: print('Dataset: ' + str(i+1) + 'year:')\n",
    "        if verbose: print('Original dataset shape {}'.format(Counter(label_dfs[i])))\n",
    "        dfi_features_res, dfi_label_res = smote.fit_resample(feature_dfs[i], label_dfs[i])\n",
    "        if verbose: print('Resampled dataset shape {}\\n'.format(Counter(dfi_label_res)))\n",
    "        # Append the resampled feature and label arrays of ith dataframe to their respective list of arrays    \n",
    "        resampled_feature_arrays.append(dfi_features_res)\n",
    "        resampled_label_arrays.append(dfi_label_res)        \n",
    "    return resampled_feature_arrays, resampled_label_arrays\n",
    "\n",
    "# Utility Function to convert the arrays of features and labels to pandas dataframes, and then join them.\n",
    "# Also re-assign the columns headers.\n",
    "def restructure_arrays_to_dataframes(feature_arrays, label_arrays):\n",
    "    resampled_dfs = []\n",
    "    for i in range(len(feature_arrays)):\n",
    "        feature_df = pd.DataFrame(data=feature_arrays[i])\n",
    "        label_df = pd.DataFrame(data=label_arrays[i])\n",
    "        # Must set the column header for label_df, otherwise it wont join with feature_df, as columns overlap (with col names '0')\n",
    "        label_df.columns=['Y'] \n",
    "        resampled_dfs.append(feature_df.join(label_df))\n",
    "    # re-assign the column headers for features and labels    \n",
    "    set_new_headers(resampled_dfs)    \n",
    "    return resampled_dfs\n",
    "\n",
    "# Perform SMOTE oversampling on all the imputed dataframes, and return them in a dictionary.\n",
    "def perform_oversampling_on_imputed_dataframes(df_dict):\n",
    "    imputed_oversampled_dataframes_dictionary = OrderedDict()\n",
    "    for key,dfs in df_dict.items():\n",
    "        print('SMOTE Oversampling for ' + key + ' imputed dataframes\\n')\n",
    "        smote_feature_arrays, smote_label_arrays = oversample_data_SMOTE(dfs, verbose=True)\n",
    "        oversampled_dataframes = restructure_arrays_to_dataframes(smote_feature_arrays, smote_label_arrays)\n",
    "        imputed_oversampled_dataframes_dictionary[key] = oversampled_dataframes\n",
    "        print('-'*100)\n",
    "    return imputed_oversampled_dataframes_dictionary\n",
    "\n",
    "imputed_oversampled_dataframes_dictionary = perform_oversampling_on_imputed_dataframes(imputed_dataframes_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NxuD0ojadig"
   },
   "outputs": [],
   "source": [
    "def prepare_kfold_cv_data(k, X, y, verbose=False):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train.append(X[train_index])\n",
    "        y_train.append(y[train_index])\n",
    "        X_test.append(X[test_index])\n",
    "        y_test.append(y[test_index])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulL73htyahod"
   },
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu5oRWhXakFg"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(penalty = 'none', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCzXXa7NbKiL"
   },
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPAF1sfUbcq1"
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DN_JpEJbcyn"
   },
   "outputs": [],
   "source": [
    "# eXtreme Gradient Boosting Classifier (XGBClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "xgb_classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Z5h4wMAb6RE"
   },
   "outputs": [],
   "source": [
    "# creating a dictionary of models\n",
    "models_dictionary = OrderedDict()\n",
    "\n",
    "models_dictionary['Gaussian Naive Bayes'] = gnb_classifier\n",
    "models_dictionary['Logistic Regression'] = lr_classifier\n",
    "models_dictionary['Decision Tree'] = dt_classifier\n",
    "models_dictionary['Extreme Gradient Boosting'] = xgb_classifier\n",
    "models_dictionary['Random Forest'] = rf_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EoX3Pm1c270"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZGhk6twb8Wq"
   },
   "outputs": [],
   "source": [
    "# perform data modeling\n",
    "def perform_data_modeling(_models_, _imputers_, verbose=False, k_folds=5):\n",
    "    \n",
    "    # 7 Models\n",
    "    # 4 Imputers\n",
    "    # 5 datasets (for 5 years)\n",
    "    # 7 metrics, averaged over all the K-Folds\n",
    "    model_results = OrderedDict()\n",
    "    \n",
    "    # Iterate over the models\n",
    "    for model_name, clf in _models_.items():\n",
    "        if verbose: print(\"-\"*120, \"\\n\", \"Model: \" + '\\033[1m' + model_name + '\\033[0m' + \" Classifier\")\n",
    "        imputer_results = OrderedDict()\n",
    "        \n",
    "        # Iterate over the different imputed_data mechanisms (Mean, k-NN, EM, MICE)\n",
    "        for imputer_name, dataframes_list in _imputers_.items():\n",
    "            if verbose: print('\\tImputer Technique: ' + '\\033[1m' + imputer_name + '\\033[0m')\n",
    "            \n",
    "            # call the split_dataframes_features_labels function to get a list of features and labels for all the dataframes\n",
    "            feature_dfs, label_dfs = split_dataframes_features_labels(dataframes_list)            \n",
    "            \n",
    "            year_results = OrderedDict()\n",
    "            \n",
    "            # Iterate over dataframe_list individually\n",
    "            for df_index in range(len(dataframes_list)):\n",
    "                if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index+1) + 'year' + '\\033[0m')\n",
    "                \n",
    "                # Calling the 'prepare_kfold_cv_data' returns lists of features and labels \n",
    "                # for train and test sets respectively.\n",
    "                # The number of items in the list is equal to k_folds\n",
    "                X_train_list, y_train_list, X_test_list, y_test_list = prepare_kfold_cv_data(k_folds, feature_dfs[df_index], label_dfs[df_index], verbose)\n",
    "                \n",
    "                metrics_results = OrderedDict()\n",
    "                accuracy_list = np.zeros([k_folds])\n",
    "                precision_list = np.zeros([k_folds,2])\n",
    "                recall_list = np.zeros([k_folds,2])\n",
    "                TN_list = np.zeros([k_folds])\n",
    "                FP_list = np.zeros([k_folds])\n",
    "                FN_list = np.zeros([k_folds])\n",
    "                TP_list = np.zeros([k_folds])                \n",
    "                \n",
    "                # Iterate over all the k-folds\n",
    "                for k_index in range(k_folds):\n",
    "                    X_train = X_train_list[k_index]\n",
    "                    y_train = y_train_list[k_index]\n",
    "                    X_test = X_test_list[k_index]\n",
    "                    y_test = y_test_list[k_index]\n",
    "                    \n",
    "                    \n",
    "                    # Fit the model and \n",
    "                    clf = clf.fit(X_train, y_train)\n",
    "                    y_test_predicted = clf.predict(X_test)\n",
    "                    \n",
    "                    #code for calculating accuracy \n",
    "                    _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n",
    "                    accuracy_list[k_index] = _accuracy_\n",
    "                    \n",
    "                    #code for calculating recall \n",
    "                    _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n",
    "                    recall_list[k_index] = _recalls_\n",
    "                    \n",
    "                    #code for calculating precision \n",
    "                    _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n",
    "                    precision_list[k_index] = _precisions_\n",
    "                    \n",
    "                    #code for calculating confusion matrix \n",
    "                    _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n",
    "                    TN_list[k_index] = _confusion_matrix_[0][0]\n",
    "                    FP_list[k_index] = _confusion_matrix_[0][1]\n",
    "                    FN_list[k_index] = _confusion_matrix_[1][0]\n",
    "                    TP_list[k_index] = _confusion_matrix_[1][1]\n",
    "                \n",
    "                # creating a metrics dictionary\n",
    "                metrics_results['Accuracy'] = np.mean(accuracy_list)\n",
    "                metrics_results['Precisions'] = np.mean(precision_list, axis=0)\n",
    "                metrics_results['Recalls'] = np.mean(recall_list, axis=0)\n",
    "                metrics_results['TN'] = np.mean(TN_list)\n",
    "                metrics_results['FP'] = np.mean(FP_list)\n",
    "                metrics_results['FN'] = np.mean(FN_list)\n",
    "                metrics_results['TP'] = np.mean(TP_list)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('\\t\\t\\tAccuracy:', metrics_results['Accuracy'])\n",
    "                    print('\\t\\t\\tPrecision:', metrics_results['Precisions'])\n",
    "                    print('\\t\\t\\tRecall:', metrics_results['Recalls'])\n",
    "                \n",
    "                year_results[str(df_index+1)+'year'] = metrics_results   \n",
    "                \n",
    "            imputer_results[imputer_name] = year_results\n",
    "            \n",
    "        model_results[model_name] = imputer_results  \n",
    "        \n",
    "    return model_results                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7k84oL2i6MJ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rry3VGnwcD2F",
    "outputId": "122e8520-59d3-4de2-99b0-a625b42a9063"
   },
   "outputs": [],
   "source": [
    "results = perform_data_modeling(models_dictionary, imputed_oversampled_dataframes_dictionary, verbose=True, k_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcMVK24h2BNR"
   },
   "outputs": [],
   "source": [
    "# model -> imputer -> year\n",
    "def perform_model_ranking(models, imputers, results):\n",
    "    column_headers = ['-'] + list(imputers.keys())\n",
    "    rows = []\n",
    "    for model_name, model_details in results.items():\n",
    "        row = [model_name]\n",
    "        for imputer_name, imputer_details in model_details.items():\n",
    "            mean_accuracy = 0\n",
    "            for year, metrics in imputer_details.items():\n",
    "                mean_accuracy += metrics['Accuracy']\n",
    "            mean_accuracy = mean_accuracy/len(imputer_details)\n",
    "            row.append(mean_accuracy)\n",
    "        rows.append(row)\n",
    "    results_df = pd.DataFrame(data=rows, columns = column_headers)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKR9gOP8bZBf"
   },
   "outputs": [],
   "source": [
    "perform_model_ranking(models_dictionary, imputed_oversampled_dataframes_dictionary, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qmpn8v3GboX9"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E0n7YFJbZBf"
   },
   "outputs": [],
   "source": [
    "file=open('rf.pkl','wb')\n",
    "pickle.dump(rf_classifier,file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8xDu8mrbZBf"
   },
   "outputs": [],
   "source": [
    "pickled_model = pickle.load(open('rf.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7J5rNwebZBf"
   },
   "outputs": [],
   "source": [
    "z = [[-18.907000,27.092000,0.040401,0.901840,1.463700,4.966700,2.364400,0.020169,0.022858,29.360000,0.620610,1.012200]]\n",
    "y = pd.DataFrame(z)\n",
    "pickled_model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxFGa_s2bwQs"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzrGoSdlpj3I"
   },
   "outputs": [],
   "source": [
    "! npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4dfRD0arvg8"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "model= pickle.load(open('rf.pkl','rb'))\n",
    "\n",
    "def run():\n",
    "    menu = ['Home','Bankruptcy Prediction App']\n",
    "    choice= st.sidebar.selectbox('Menu',menu)\n",
    "    if choice== 'Home':\n",
    "      st.title(\"Corporate Bankruptcy\")\n",
    "      st.header('What is Bankruptcy?')\n",
    "      st.write('Bankruptcy is the inability of a person or a buusiness to pay their debt.')\n",
    "      st.write('It involves the sale of property and some other arrangement to pay as mush as possible of the ,oney of a person or business entity owe.')\n",
    "      st.write('Bankruptcy is a legal proceeding involving a person or business that is unable to repay their outstanding debts.')\n",
    "      st.write(' The bankruptcy process begins with a petition filed by the debtor, which is most common, or on behalf of creditors,which is less common.')\n",
    "      st.write('All of the debtors assets are measured and evaluated, and the assets may be used to repay a portion of outstanding debt.')\n",
    "      st.subheader('About the Machine Learning Model and Developer')\n",
    "      st.markdown('''### Model Algorithm : Random Forest Classifier''')\n",
    "      st.markdown('''### Model Accuracy: 94.5746%''')\n",
    "      st.markdown('''## Model Developed by Karan Singh Thakur, Ridhhi Aggarwal''')\n",
    "      st.markdown('''### contact: kthakur1_be20@thapar.edu, raggarwal1_be20@thapar.edu''')\n",
    "      \n",
    "    elif choice== 'Bankruptcy Prediction App':\n",
    "      st.title(\"Corporate Bankruptcy prediction Model\")\n",
    "      st.subheader(\"This model will predict if a company would Bankrupt in future or not\")\n",
    "      #Company Name:\n",
    "      comp_name= st.text_input('Company Name')\n",
    "    # [(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] 365\n",
    "      attr5= st.number_input(' [(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] 365')\n",
    "    #(gross profit + depreciation) / sales\n",
    "      attr13= st.number_input('(gross profit + depreciation) / sales')\n",
    "    #(inventory * 365) / sales\n",
    "      attr20= st.number_input('(inventory * 365) / sales')\n",
    "    #profit on operating activities / total assets\n",
    "      attr22= st.number_input('profit on operating activities / total assets')\n",
    "    #gross profit (in 3 years) / total assets\n",
    "      attr24=st.number_input('gross profit (in 3 years) / total assets')\n",
    "    #profit on operating activities / financial expenses\n",
    "      attr27= st.number_input('profit on operating activities / financial expenses')\n",
    "    #logarithm of total assets\n",
    "      attr29= st.number_input('logarithm of total assets')\n",
    "    #operating expenses / total liabilities\n",
    "      attr34= st.number_input('operating expenses / total liabilities')\n",
    "    #(current assets - inventory - receivables) / short-term liabilities\n",
    "      attr40= st.number_input('(current assets - inventory - receivables) / short-term liabilities')\n",
    "    #(receivables * 365) / sales\n",
    "      attr44= st.number_input('(receivables * 365) / sales')\n",
    "    #(current assets - inventory) / short-term liabilities\n",
    "      attr46= st.number_input('(current assets - inventory) / short-term liabilities')\n",
    "    #total costs /total sales\n",
    "      attr58= st.number_input('total costs /total sales')\n",
    "    #''Attr5', 'Attr13', 'Attr20', 'Attr22', 'Attr24', 'Attr27', 'Attr29',\n",
    "       #'Attr34', 'Attr40', 'Attr44', 'Attr46', 'Attr58\n",
    "      features= [[attr5,attr13,attr20,attr22,attr24,attr27,\n",
    "                attr29,attr34,attr40,attr44,attr46,attr58 ]]\n",
    "      print(features)\n",
    "      ans = y = model.predict(features)\n",
    "      if st.button(\"Predict\"):\n",
    "        if ans == 0:\n",
    "          st.warning(\"Your Company \" + comp_name + ' is in low risk for bankruptcy.')\n",
    "        else:\n",
    "          st.success(\"Your Company \" + comp_name + ' is in high risk for bankruptcy.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBMS897ZetxP",
    "outputId": "2fb1fe18-abec-4caa-bd38-aa6cc7a60595"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiymHtltsYmH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH5LnOK3veW-"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
